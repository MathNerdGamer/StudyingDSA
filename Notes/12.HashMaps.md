# Hash Maps

* Note that we currently have data structures that achieve $O\left(\lg n\right)$ runtimes for storing and accessing data. Can we beat that?

* If we allow for some probability, we can actually achieve $O\left(1\right)$ in expectation, using something called *hashing* to assign indices of an array into which we store our data!
    * Why do we need probability? - We might have some terrible luck in how we've selected our hashing method vs the data we store, where we might accidentally assign the same index to many different items, which is called a *collision*.
    * As a result, we need to know how likely something like that happens (which depends on our hashing scheme), as well as how we can best handle such situations.

## Map ADT

* A *map* is a collection of key-value pairs $\langle K, V\rangle$ that are searchable.
    * A map may be *ordered* or *unordered*.
    * In this course, we'll be looking at unordered maps by using hashing.
    * A map may also sometimes be called a *dictionary*.

* Maps use the keys to store and retrieve the associated values.

* We want keys to be unique and immutable. The values that the keys are assigned to, however, may have multiple copies and may change.

* The *Map ADT* has the following methods:
    * `V put(K, V)`: Follows the key to an entry to either add or replace a value. If a value is replaced, then it is returned, otherwise `null` is returned.
    * `V get(K)`: Follows the key to an entry and returns the value stored there, or returns `null` if there's nothing there.
    * `V remove(K)`: Same as `get`, but removed the key-value pair.
    * `List<V> values(K)`: Returns the array of values in no particular order.
    * `int size()`: Returns the size of the number of entries.

* With a backing array and a way to assign a unique index into that array as the key for our entries, we would have $O\left(1\right)$ access.

* To facilitate this, a *hashing function* is used.
    * A hashing function takes an object and returns an integer value (the *hash code*), and is given by a `hashcode()` method.
    * With this hash code, we then get an index into the backing array by processing it so that it fits into the correct bounds.
    * However, while it's true that if `A.equals(B)` is true, then `A.hashcode() == B.hashcode()`, the converse is not necessarily true.

* A very basic example of how one might design a hash function, imagine a location with multiple departments that each have a phone number. Our hash code could be taken as the last four digits of the phone number, but then we'd need an array of size `10000` to store what could be less than 100 numbers, as we may have numbers like `1234`.
    * The most common solution is to take this modulo the capacity of the backing array.
    * An issue with this is that we may have two numbers who give the same result when doing this operation. For example, `1234` and `2134` both give `34` when we have an array of size 100.
    * This is a collision. Collisions are practically unavoidable.
    * We will see many different ways to handle collisions in later sections, but we can also try to avoid collisions.

* To avoid collisions, we typically resize when we have "too few" empty entries.
    * When resizing, we typically do the following: `capacity = 2 * capacity + 1`
    * We don't wait until the array is full, but instead resize when we reach some threshold, called the *maximum load factor*.
        * The *load factor* is defined by `loadFactor = (float)size / capacity`.
        * Typical maximum load factors for resizing are between 60% and 70%.
    * Sometimes we use prime numbers for the backing array capacity.
        * This can minimize collisions when the data we want to store exhibits some particular patterns.

* There's the related *Set ADT* which acts very similarly to the *Map ADT*, but where the keys are, themselves, the values.

## Hash Map Implementation Considerations

* In Hash Maps, collisions are the source of most inefficiencies, and so we will spend a lot of time trying out different collision resolution strategies.

* However, before that, we need a good foundation that will help mitigate some collisions. This means selecting hash functions, compression functions, table sizes, maximum load factors, etc.

* Formally, if we call the set of keys $K$, a hash function is $h: K\to\mathbb{Z}$. That is, for each key, we map it to some integer.
    * We *really* want this function to be one-to-one (injective). That is, was *really* want $h\left(k\\_{1}\right) = h\left(k\\_{2}\right)$ to guarantee $k\\_{1} = k\\_{2}.$
    * In practice, however, this may not be practical to guarantee (or even possible, in some instances).
    * The main issue is that, while we claim that the range is the set of integers, we really can only support a finite range, since arbitrarily large integers would require an arbitrarily large amount of memory to store.
    * This means that, technically, we *could* have a set of keys so large that injectivity is *impossible*.

* When it comes to compression functions, the most common, by far, is simply taking the hash code modulo the table size. So, how do we decide on table sizes?
    * It's very common to use prime numbers for the table size. This can help with preventing collisions, but modular arithmetic by primes can be computationally expensive.
    * It's also very common to use powers of 2, since it's very easy to do modular arithmetic (we just mask the higher-order bits out). However, like the phone number example, this opens us up to many collisions.

* For load factors, we need to balance, taking our particular collision handling into account, the amount of risk we want to take for collisions (higher load factor means more likely collisions) and how much memory footprint we want (lower load factors require larger backing arrays).

* There is a lot of theoretical and practical work put into Hash Maps. Some other courses to consider for follow up after this one:
    * [MIT 6.006](https://ocw.mit.edu/courses/6-006-introduction-to-algorithms-spring-2020) - Lecture 6 covers what we'll be seeing about hashing, but with a little more theoretical coverage.
    * [MIT 6.046J](https://ocw.mit.edu/courses/6-046j-design-and-analysis-of-algorithms-spring-2015/) - Lecture 8 goes even further in the theoretical direction.
    * [Harvard CS224](https://people.seas.harvard.edu/~cs224/fall14/lec.html) - Lecture 3 covers some of the same stuff, but talks about $k$-wise independence and particular values of $k$ where linear probing (one of the strategies we'll see later for handling collisions) achieves expected $O\left(1\right)$ runtime.

**Next: [Collision Handling](./13.CollisionHandling.md)**