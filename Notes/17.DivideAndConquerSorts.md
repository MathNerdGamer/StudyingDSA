# Divide & Conquer Sorting Algorithms

* Previously, we've been looking at iterative sorting algorithms.

* However, these have all had a quadratic worst-case number of comparisons. Can we do better?

* A *divide & conquer algorithm* breaks a problem down into smaller instances which can then be solved and put together into a solution to the original problem.

* A *divide & conquer sorting algorithm*, then, breaks our array up into subarrays to sort and put back together into a fully sorted array.
    * Many such algorithms use recursion.

## Merge Sort

* With merge sort, we recursively split the array inside subarrays, and then merge them back together in order.
    * This can be costly, as this involves allocating extra space to store the left/right halves. 
    * There *are* in-place versions of merge sort, but they are beyond the scope of this course.

* Pseudocode for Merge Sort:
    ```java
    public static void mergeSort(int[] array) {
        mergeRecurse(array, 0, array.length - 1);
    }

    private static void mergeRecurse(int[] array, int left, int right) {
        int mid = left + (right - left) / 2;

        if (left < right) {
            mergeRecurse(array, left, mid);
            mergeRecurse(array, mid + 1, right);
            merge(array, left, mid, right);
        }
    }

    private static void merge(int[] array, int left, int mid, int right) {
        // Calculate subarray sizes
        int leftSize = mid + 1 - left;
        int rightSize = right - mid;

        // Allocate subarrays.
        int[] leftSubarray = new int[leftSize];
        int[] rightSubarray = new int[rightSize];

        // Store subarrays
        for (int i = 0; i < leftSize; i++) {
            leftSubarray[i] = array[left + i];
        }

        for (int j = 0; j < rightSize; j++) {
            rightSubarray[j] = array[mid + 1 + j];
        }

        int i = 0;
        int j = 0;
        int k = left;

        // Merge.
        while (i < leftSize && j < rightSize) {
            if (leftSubarray[i] < rightSubarray[j]) {
                array[k] = leftSubarray[i];
                i++;
            } else {
                array[k] = rightSubarray[j];
                j++;
            }

            k++;
        }

        // Append any remainders from the subarrays at the end.
        while (i < leftSize) {
            array[k] = leftSubarray[i];
            i++;
            k++;
        }

        while (j < rightSize) {
            array[k] = rightSubarray[j];
            j++;
            k++;
        }
    }
    ```

* Note that only one of the two while loops at the end will actually do anything, as we only go inside of one if the other's subarray has been fully merged in.

**Theorem.** Merge Sort is correct.

**Theorem.** The best-, worst-, and average-case number of comparisons for merge sort is $\Theta\left(n \lg n\right)$.

**Proposition.** Merge Sort is stable, non-adaptive, and out-of-place.

## Quick Sort

* There are a few variants of quick sort, so we'll discuss it generically at first, but then we'll choose a couple of variants when analyzing it.
    * We *will* be doing in-place quick sort.
    * The variants we will analyze use the Hoare partition scheme (one deterministic, one randomized).
    * The [Sorting Algorithm Visualizer](../sorting/README.md) code (written in C using SDL) contains multiple variants.

* The idea behind quick sort is that we pick an element in the array, called a *pivot*, and move elements less than the pivot to the left and elements greater than the pivot to the right so that the pivot ends in the correct final position.

* Once this is done, we recurse on the left and right subarrays.

* The real magic comes from how we choose the pivot and how we go about rearranging things.

* The following is Java code for two variants that we'll be analyzing, one is the "Basic Quick Sort" and the other "Randomized Quick Sort":
```java
public static void quickSort(int[] array) {
    quickRecurse(array, 0, array.length - 1, false);
}

public static void randomizedQuickSort(int[] array) {
    quickRecurse(array, 0, array.length - 1, true);
}

private static void quickRecurse(int[] array, int hi, int lo, boolean random) {
    if (lo >= hi || lo < 0 || hi < 0) {
        return;
    }

    int pivot = partition(array, lo, hi, random);

    quickRecurse(array, lo, pivot);
    quickRecurse(array, pivot + 1, hi);
}

private static void swap(int[] array, int i, int j) {
    int temp = array[i];
    array[i] = array[j];
    array[j] = temp;
}

private static void randomSwap(int[] array, int lo, int hi) {
    Random rand = new Random();

    int randomIndex = lo + (rand.nextInt(hi - lo));
    swap(array, lo, randomIndex);
}

private static void partition(int[] array, int lo, int hi, boolean random) {
    if (random) {
        randomSwap(array, lo, hi);
    }

    int pivot = array[lo];

    int i = lo - 1;
    int j = hi + 1;

    while (true) {
        do {
            i++;
        } while (array[i] < pivot);

        do {
            j--;
        } while (array[j] > pivot);

        if (i >= j) {
            return j;
        }

        swap(array, i, j);
    }
}
```

* When we analyze these variants of Quick Sort, the only part that will actually matter is how we're choosing a pivot.

**Theorem.** Both variants of Quick Sort are correct.

**Theorem.** The worst-case number of comparisons for basic quick sort is $O\left(n^{2}\right)$, but $O\left(n \lg n\right)$ in expectation for randomized quick sort.

**Theorem.** The best- and average-case number of comparisons for basic and randomized quick sort is $O\left(n \lg n\right)$.

**Proposition.** Quick Sort is unstable, non-adaptive, and in-place.

**Next: [Analyses of Divide & Conquer Sorting Algorithms](./18.DivideAndConquerSortAnalyses.md)**