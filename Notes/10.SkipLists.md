# Skip Lists & Randomization in Computing

## Skip Lists

* A *skip list* is a data structure comprised of multiple levels of lists.

* As the levels go up, the lists becomes sparser, and the moving from left to right skips multiple nodes that would have had to be traversed at lower levels.

* Nodes hold data, as well as pointers `left`/`right` and `up`/`down`.

* Each level has a $-\infty$ node as the `head` and an $\infty$ node as a `tail`.

* The `head` of the overall skip list is the top-most $-\infty$ node.

* Level $i$ contains a subset of data from level $i-1$, with level $0$ holding all of the data.

* When an element is inserted into a skip list, it is first inserted at level $0$.

* For each $i$, if an element is inserted at level $i$, there's also a $\displaystyle{\frac{1}{2}}$ probability (a fair coin flip) that it will be promoted to the next level, which means it *also* gets inserted at level $i+1$.
    * When this happens, the node on level $i$ sets its `up` pointer to its counterpart in level $i+1$, and the counterpart points back with its `down` pointer.
    * The counterpart node also links it's `left`/`right` pointers to the nodes on either side on its level.

* The probability of a node reaching level $i$ is $\displaystyle{\frac{1}{2^{i}}}$, as it would need to be promoted from level $0$ to $1$, from $1$ to $2$, and so on, until it reaches level $i$, which requires $i$ *heads* in a row when flipping a fair coin.

* The expected number of levels is $O\left(\lg n\right)$, and some implementations cap the levels to $\lg n$ to prevent an outlier event where the coin flips result in heads far too many times.

* Since each level, in expectation, has half the elements of the previous level, this means the expected space complexity is $O\left(n\right)$ (since $\displaystyle{\sum_{k\geq 0} \frac{1}{2^{k}} = 2}$).

* When searching for an element:
    * Start from the `head` (top-most $-\infty$ node).
    * Compare the target `data` to the node to the right of the current node.
        * If `data > node.right.data`: move right
        * If `data < node.right.data` or `node.right == null`: move down (or return `false` if at level $0$)
        * If `data == node.right.data`: return `true`

* When adding an element, do the same steps as above, but if the element is *not* found, then we know where to insert.
    * Remember to do the coin flips to promote.

* When removing an element, we again search for the data.
    * If the data is found, then we'll be on the top-most level that contains the node holding that data, call it level $k$.
    * We temporarily store the `down` pointer, remove the node from the level $k$ list as in a usual linked list.
    * Then, we repeat the previous step at levels $k-1$ down to $0$.

* The complexities of a skip list are as follows:
    * Worst case $O\left(n\right)$ for all operations and expected $O\left(\lg n\right)$ for all operations.
    * Worst case $O\left(n \lg n\right)$ for space, in the case where every node is promoted to all levels (assuming a $\lg n$ level cap), and expected $O\left(n\right)$ space.

## Randomization in Computing

* Using randomization has, historically, aided algorithm developers with finding more efficient algorithms than with *deterministic* (non-randomized) methods.

* While it is conjectured that randomization doesn't produce *much* more efficient algorithms, meaning that it's believed that any problem solvable in polynomial time using randomization can also be solved in deterministic polynomial time ([this is the so-called $P$ vs $BPP$ conjecture, which is one of the very rare cases of *equality* of the two complexity classes being the predominant conjecture](https://en.wikipedia.org/wiki/BPP_(complexity)#Derandomization)), it is likely that there may be at least *some* speed up when using randomization (which can make it possible to solve some problems that would, otherwise, be practically intractable).
    * An example near to my heart, primality testing, is one such case. The [AKS algorithm](https://en.wikipedia.org/wiki/AKS_primality_test) is a deterministic poly-time primality test, but it takes $\widetilde{O}\left(n^{6}\right)$ (here, $n$ is the bit-length of the number being tested, and $\widetilde{O}$ means logarithmic factors are hidden) using the most efficient known variant.
    * On the other hand, the [Miller-Rabin algorithm](https://en.wikipedia.org/wiki/Miller%E2%80%93Rabin_primality_test), which uses randomization, runs in $\widetilde{O}\left(k n^{3}\right)$ steps, where $k$ is the number of rounds (additional rounds decreases the probability of failure exponentially -- specifically, the probability of a false positive is $\displaystyle{\frac{1}{4^{k}}}$ after $k$ rounds).
    * While $\widetilde{O}\left(n^{6}\right)$ *is* a polynomial runtime, it is not feasible in practice -- however, $\widetilde{O}\left(k n^{3}\right)$ is (just barely) tractable.

* In theoretical analyses, we assume access to a (*true*) *random number generator*, or *(T)RNG*. However, in practice, we only really have access to what is known as a *pseudo-*random number generator, or *PRNG*. It turns out that these PRNGs, when properly used, seem to work well enough, in practice.
    * In fact, one avenue to proving $P = BPP$ is by way of exhibiting a (provably) "*good enough*" PRNG to facilitate derandomization.

**Next: [Binary Heaps](./11.BinaryHeaps.md)**