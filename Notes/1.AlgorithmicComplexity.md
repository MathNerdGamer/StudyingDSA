# Algorithmic Complexity
* When studying algorithms, we want to be able to measure time and space efficiency.
* We want this measurement to be independent of the platform and hardware used.
* We also want this measurement to be a measure of some high-level description of our algorithm.
    * In particular, we want to define our measurements in terms of some primitive operations, such as:
        * Value assignment
        * Arithmetic operations
        * Comparisons
        * Method calls
        * Element access
        * Derefencing of pointers
    * To simplify analyses a bit, primitive operations are measured as taking constant time.
    * As a result, runtime is measured as the number of primitive operations.
* We want performance measures to scale in terms of the size of the input to our algorithm.
* The *complexity* of an algorithm is how efficient it is in terms of input sizes, and usually measures *runtime* or *space*.
* We use function notation, such as $f\left(n\right)$, to denote the number of primitive operations our algorithm uses for each input size $n$.
* There are three basic measures that this function could be modeling:
    * **Worst-case runtime**: The function represents how many primitive operations our algorithm uses when given its worst possible input for each size $n$. In a more mathematical notation, if we call our algorithm $A$ and denote its runtime on input $z$ by $A\left(z\right)$, this is $f\left(n\right) \coloneqq \sup\limits_{|z|=n}\left\lbrace A\left(z\right)\right\rbrace$.
    * **Best-case runtime**: The function represents the lowest number of primitive operations our algorithm uses for any input of size $n$. Mathematically, this is $f\left(n\right) \coloneqq \inf\limits_{|z|=n}\left\lbrace A\left(z\right)\right\rbrace$.
    * **Average-case runtimes**: This function represents the expected number of primitive operations that our algorithm uses for inputs of size $n$. Note that this actually requires defining some probability distribution over inputs, though one typically takes the uniform distribution. Mathematically, if we let $D_{n}$ denote the probability distribution over inputs of size $n$ for our algorithm, then this is $f\left(n\right) \coloneqq \mathop{\mathbb{E}}\limits_{z{\sim}D_{n}}\left[A\left(z\right)\right]$.
* The following tables gives an idea of how long the runtime would be for some common functions:

    | Algorithm Runtime | Time elapsed on 100 Hz machine for $n = 1000$ | Time elapsed on 10 GHz machine for $n = 1\text{ million}$ |
    | :------------------: | :-------------------------------------------: | :-------------------------------------------------------: |
    | 1                    | 10 ms                                         | 0.1 ns                                                    |
    | $\lg n$              | ~100 ms                                       | 2 ns                                                      |
    | $n$                  | 10 seconds                                    | 0.1 ms                                                    |
    | $n \lg n$            | 1-2 mins                                      | 2 ms                                                      |
    | $n^{2}$              | 2-3 hours                                     | 1-2 mins                                                  |
    | $n^{3}$              | 115 days                                      | > 3 years                                                 |
    | $2^{n}$              | Heat death of the universe                    | Heat death of the universe                                |

* Because the exact number of primitive operations that an algorithm uses is never so clean, we must introduce some extra notation, called *asymptotic notation*, to simplify the expressions we study to those like in the above table.

* There are multiple asymptotic notations, but the following are the most common in computer science:
    * (Big-Oh) We say that a function $g\left(n\right)$ is $O\left(f\left(n\right)\right)$ iff there exists a constant $C$ and an integer $N > 0$ such that $g\left(n\right)\leq C f\left(n\right)$ for all $n\geq N$. This says that $f\left(n\right)$ is an *upper bound* for $g\left(n\right)$. This is the most common asymptotic notation, and we usually want to reserve this for *sharp* upper bounds.
    * (Little-oh) We say that a function $g\left(n\right)$ is $o\left(f\left(n\right)\right)$ iff $\lim\limits_{n\to\infty}\frac{g\left(n\right)}{f\left(n\right)} = 0$. In other words, $f\left(n\right)$ is a **significant** upper bound for $g\left(n\right)$.
    * (Big-$\Omega$) We say that a function $g\left(n\right)$ is $\Omega\left(f\left(n\right)\right)$ iff there exists a constant $C$ and an integer $N > 0$ such that $g\left(n\right)\geq C f\left(n\right)$ for all $n\geq N$. This says that $f\left(n\right)$ is a *lower bound* for $g\left(n\right)$.
    * (Little-$\omega$) We say that a function $g\left(n\right)$ is $\omega\left(f\left(n\right)\right)$ iff $\lim\limits_{n\to\infty}\frac{f\left(n\right)}{g\left(n\right)} = 0$. In other words, $f\left(n\right)$ is a **significant** lower bound for $g\left(n\right)$.
    * (Big-$\Theta$) We say that a function $g\left(n\right)$ is $\Theta\left(f\left(n\right)\right)$ iff $g\left(n\right)$ is both $O\left(f\left(n\right)\right)$ *and* $\Omega\left(f\left(n\right)\right)$. This means that both functions grow at roughly the same rate as each other, up to some constants.
    * ("Asymptotic To") We say that $f\left(n\right)\sim g\left(n\right)$ iff $\lim\limits_{n\to\infty}\frac{f\left(n\right)}{g\left(n\right)} = 1$.

* By convention, we drop all constant factors and lower-order terms when using the first five asymptotic notations, since the contribution of these constant factors and lower-order terms tend to be insignificant to the overall growth of an algorithm's runtime. The only exception is that the main term retains any constant factors for the $\sim$ notation, since changing it will affect the limit.

* This means we can simplify the algorithm's runtime to be characterized by some functions we already know, like those seen in the table above. The following table gives some names to the growth rates associated to these functions:

    |  Function  |  Name for the Growth Rate  |
    | :--------: | :------------------------: |
    | 1          | Constant                   |
    | $\lg n$    | Logarithmic                |
    | $n$        | Linear                     |
    | $n \lg n$  | Quasilinear                |
    | $n^{2}$    | Quadratic                  |
    | $n^{3}$    | Cubic                      |
    | $2^{n}$    | Exponential                |

* Note that this simplification does leave some edge cases. Generally, we'll regard an algorithm with $O\left(n \lg n\right)$ runtime as "better" than an algorithm with $O\left(n^{2}\right)$ runtime for the same problem, but the constant factors will be significant whenever $n$ is small enough. This will be talked about a bit when it becomes relevant later on.

**Next: [List ADT](./2.ListADT.md)**