# Amortized Analysis of Dynamic Arrays
* When adding elements to a dynamic array, one occasionally gets hit with an $O\left(n\right)$ resize operation.

* This operation occurs whenever the array is full and a new array needs to be allocated, which forces elements from the old array to be copied over.

* However, this $O\left(n\right)$, while technically true, is somewhat misleading. It doesn't happen every time an element is added, so maybe we can "charge" the big operation to all of the small ones by averaging the total cost over a sequence of operations. This is known as *amortized analysis*.

**Theorem.** The insertion of an element into a dynamic array has $\Theta\left(1\right)$ *amortized* runtime, given a resize policy which doubles the size of the array when resizing.

**Proof.** Let $A$ be an array with capacity $n$ and consider a sequence of $n$ insertion operations into a dynamic array.

There are two cases:

* Case 1 [No Resize]: If no resize is necessary, then each operation has a cost of $1$, which gives a total cost of $n$. Averaging this over the $n$ operations gives an amortized $\Theta\left(1\right)$ runtime.

* Case 2 [A Resize Occurs]: Suppose a resize occurs after some number of insertions. This resize costs $n$ due to copying all elements of the old array into the newly allocated one, and only occurs once since we're making space for at least $n$ more elements. The insertions, themselves, each cost $1$, and so the total cost of the operations. When averaging over the $n$ operations, this gives an amortized cost of $\frac{2n}{n} = 2 = \Theta\left(1\right)$.

**QED**

* Note that, technically, as long as the resize operation multiplies the capacity by some factor greater than 1, this result still holds. The proof of this is only ever-so-slightly more involved, since we don't guarantee only one resize (though the total number will be $\Theta\left(1\right)$).

* In fact, some dynamic array implementations, such as `std::vector` from Visual Studio's C++ implementation, use a factor of 1.5 instead of 2. This can save quite a bit of space while maintaining a high performance in practice, since doubling could potentially lead to an array that is much larger than needed.

* On the other hand, a factor that's too small, like $1.01$, while theoretically keeping an $\Theta\left(1\right)$ amortized runtime, would tend to reallocate a little too often in practice when insertions happen often.

* Therefore, getting the correct balance between space usage and actual runtime is a balancing act that language and library implementers need to be aware of.

* Why do we need to multiply by a factor rather than simply adding some number? The following proposition shows that adding a constant to an array size means that, eventually, you'll spend more than constant time on sequences of insertions.

**Proposition.** The insertion of an element into a dynamic array has an $\omega\left(1\right)$ amortized runtime, given a resize policy which adds some constant to the size of the array.

**Proof.** Suppose the constant is $k$. Let $A$ be a dynamic array of size $n$ and consider the following sequence of insertions:

First, we insert $n$ items. These each cost $\Theta\left(1\right)$.

Each $k$ items we add from now on requires the $k$ insertions and resize operations that take $\Theta\left(n + tk\right)$ time each, where $t$ is the number of times we've added $k$. This gives, after adding $k$ items $N$ times, a lower bound cummulative cost of

$$\Omega\left(n + Nk + \sum_{t = 0}^{N-1}n + tk\right) = \Omega\left(Nn + N^{2}k\right).$$

Since we'll have added a total of $n + Nk$ items, the amortized runtime is
$$\frac{\Omega\left(Nn + N^{2}k\right)}{n + Nk} = \Omega\left(N\right).$$

Letting $N = \omega\left(1\right)$ gives the result.

**QED**

The $\omega\left(1\right)$ can be thought of as $n$, if one would prefer a more concrete result. In that case, we get an amortized runtime of $\Theta\left(n\right)$.

**Next: [Recursion](./4.Recursion.md)**