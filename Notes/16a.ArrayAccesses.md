# Array Accesses of Iterative Sorting Algorithms

## A Detour From The Comparison Model

When studying sorting algorithms, we usually consider the comparison model. However, comparisons aren't the only thing that costs performance in a sorting algorithm, especially if we consider rather large sets of data where accessing parts of the array could incur cache misses. So, what can we say about the number of accesses for our iterative sorting algorithms?

* Instead of trying to get a general formula for all of them, here we'll only worry about the case where we're sorting an array that is reversed.

* I believe it would be interesting to see how to generalize this to any permutation of $n$ items, but it would require more than simple parameterizing by the number of inversions.

* In the following, note that swaps require 4 array accesses, as we need to access the array to store into a `temp` variable, access twice to set one entry to the second, and then access one more time to set the other entry to `temp`.

### Bubble Sort

**Theorem.** When sorting an array of $n$ distinct elements in reverse order, bubble sort accesses the array exactly $3n\left(n-1\right)$ times.

**Proof.**

Since our array is in reverse order, each step has both a comparison (2 accesses) and a swap (4 accesses), contributing a total of 6 array accesses.

Since the number of swaps is equal to the number of inversions, there are a total of $\frac{n\left(n - 1\right)}{2}$ swaps made by bubble sort by the end of its execution.

Hence, there are exactly $6\cdot \frac{n\left(n - 1\right)}{2} = 3n\left(n - 1\right)$ array accesses.

**QED**

### Insertion Sort

**Theorem.** When sorting an array of $n$ distinct elements in reverse order, insertion sort accesses the array exactly $\frac{\left(3n + 4\right)\left(n - 1\right)}{2}$ times.

**Proof.**

Insertion sort's array accesses happen in 3 phases (on iteration $j\in \left\lbrace 1,\ldots, n \right\rbrace$):

1. Store $A\left[j\right]$ in a variable $x$, then set $i = j - 1$.
2. Iteratively, `while (i >= 0 && A[i] >= x)`:
    * If we enter the while loop, set $A\left[i + 1\right]$ equal to $A\left[i\right]$.
3. When the inner loop finishes, set $A\left[i + 1\right]$ equal to $x$.

Steps 1 and 3 together contribute 2 array accesses for each iteration of the outer loop, and so we get a total of $2\left(n - 1\right)$ over the course of the algorithm from these two steps.

Since our array is in reverse order, the conditional `A[i] >= x` always evaluates to true, so we get $3j$ array accesses on iteration $j$, one for the conditional and two for the assignment inside the while loop.

Therefore, the total number of array accesses is

$$3\left(\sum_{1\leq j\leq n - 1} j\right) + 2\left(n - 1\right) = \frac{3n\left(n - 1\right)}{2} + 2\left(n - 1\right) = \left(\frac{3}{2}n + 2\right)\left(n - 1\right) = \frac{\left(3n + 4\right)\left(n - 1\right)}{2},$$

as required.

**QED**

### Selection Sort

*Warning:* You might want to get a few sheets of paper and start simulating selection sort a bit and counting the number of array accesses for this next proof. I simulated it for $n = 1\ldots 10$ and it helped a lot.

**Theorem.** When sorting an array of $n$ distinct elements in reverse order, selection sort accesses the array exactly $n^{2} + n - 1 + \left(-1\right)^{n}$ times.

**Proof.**

We proceed by induction. Here, we'll need two base cases to account for the parity of $n$, because we'll be going from $n$ to $n+2$ in our inductive case.

Base Case 1 ($n = 1$): In this case, selection sort accesses the array $0 = 1^{2} + 1 - 1 + \left(-1\right)^{1}$ times.

Base Case 2 ($n = 2$): In this case, selection sort accesses the array $6 = 2^{2} + 2 - 1 + \left(-1\right)^{2}$ times.

Inductive Case: Suppose this result holds for all integers up to some $n \geq 2$. When sorting an array of $n+2$ distinct elements in reverse order, selection sort loops over an index variable $i$ from 0 to $n$.

Section sort's first iteration's array accesses happen in 3 phases:

1. A variable `minIndex` is assigned the value of `0`.
2. Iteratively, `for 1 <= j <= n+1:`
    * Compare $A\left[j\right]$ with $A\left[\rm{minIndex}\right]$.
    * If it's less, set `minIndex` to `j`.
3. Once the inner loop finishes, if `minIndex != 0`, swap $A\left[0\right]$ with $A\left[\rm{minIndex}\right]$.

Since our elements are in reverse order, the conditional inside the inner loop always evaluates to true, and so we will end up swapping.

Step 1 costs 0 array accesses, step 2 costs $2\left(n+1\right)$ array accesses, and step 3 costs 4 array accesses, for a total of $2\left(n+3\right)$ in the first iteration of the outer loop. This first iteration puts the smallest and largest elements of the array in the correct positions, leaving $n$ unsorted elements in the middle, and so we may use the inductive hypothesis for the middle portion.

However, on each iteration after the first, we also incur some "unnecessary" accesses, as our inner loop always increases the index to $n+1$, even once the array's already sorted. Since the inductive hypothesis already accounts for the middle portion, that means we only incur an extra (thus far unaccounted for) 2 accesses for each iteration of the outer loop given by the extra element in the right-most position. Since there are $n$ iterations left after the first, this gives $2n$ more.

Hence, we incur a total of

$$2\left(n+3\right) + \left(n^{2} + n - 1 + \left(-1\right)^{n}\right) + 2n = n^{2} + 5n + 5 + \left(-1\right)^{n}$$

array accesses.

Then, there are two cases depending on the parity of $n$.

*Case 1* ($n$ even): When $n$ is even, $\left(-1\right)^{n} = 1$, and so we get

$$n^{2} + 5n + 5 + \left(-1\right)^{n} = n^{2} + 5n + 6 = \left(n^{2} + 4n + 4\right) + \left(n + 2\right)$$

$$= \left(n+2\right)^{2} + \left(n+2\right) = \left(n+2\right)^{2} + \left(n+2\right) - 1 + \left(-1\right)^{n},$$

as required.

*Case 2* ($n$ odd): When $n$ is odd, $\left(-1\right)^{n} = -1$, and so we get

$$n^{2} + 5n + 5 + \left(-1\right)^{n} = n^{2} + 5n + 4 = \left(n^{2} + 4n + 4\right) + \left(n + 2\right) - 2$$

$$= \left(n+2\right)^{2} + \left(n+2\right) - 2 = \left(n+2\right)^{2} + \left(n+2\right) - 1 + \left(-1\right)^{n}.$$

Therefore, by induction, we're finished.

**QED**

* You may also be wondering where I found the selection sort formula, since it seems much less natural.
    * The answer: Lots of paper and [OEIS](https://oeis.org).
    * In particular, if we call the selection sort accesses $s\left(n\right)$, then $s\left(n\right) = 2\cdot$[A176222(n+2)](https://oeis.org/A176222).
    * *Always* use OEIS for stuff like this. It's one of the main tools in [street fighting mathematics](https://www.youtube.com/watch?v=qP4XEZ54eSc).

## Was This Necessary?

* If you stop to think about it, nobody would use one of these iterative algorithms on data sets large enough for accesses to hurt performance *that* much.

* But, as a counter point, wasn't it interesting?

* This note is work I did when working on the [Sorting Algorithm Visualizer](../sorting/README.md). I wanted to analyze the number of accesses to verify that I had correctly instrumented the sorting algorithms in my implementations. Whether these theorems are "new" or not, I wasn't able to find anything in the literature, though I admit I didn't spend too much time looking.

**Next: [Bogo Sort](./16b.BogoSort.md)**