# Iterative Sorts

* Now that we've seen some data structures, we're going to look at some algorithms.

* An *algorithm*, informally, is a finite sequence of steps that solve a computational problem.
    * Algorithms take inputs and produce an output (true/false, some desired change in state, or a numerical result).

* A *sorting algorithm* is a particular kind of algorithm which takes a collection of comparable elements and puts them in ascending order according to some order (typically the "less-than" relation for whichever type).
    * An *iterative* sorting algorithm, then, is one which takes some number of passes over the data (iterations), and swaps them in such a way that the final result is a sorted array.

* We introduce the following *model of computation* (which is some way of measuring "steps" in an algorithm), the *comparison model*.
    * In the comparison model, we only care about measuring the number of comparisons are necessary to sort the data.
    * This is because, except for some particular situations, we can't avoid using comparisons when sorting.
    * However, we also can't hold a sorting algorithm responsible for the runtime caused by the comparisons, themselves, as there are some structures where a single comparison can take a non-trivial amount of time.

* One property of sorting algorithms is *stability*.
    * A *stable* sorting algorithm maintains the relative order of duplicate elements.
    * An *unstable* sorting algorithm, then, may swap the relative positions of duplicate elements because of some non-adjacent swap.
    * Stable sorts are important when ordering based on multiple attributes.
        * For example, suppose we want to score high scores in a video game.
        * Of course, we want higher scores at the top, but suppose we also want to weigh quicker games that got the same score more to reward players for their time efficiency.
        * If we first sort by the play time and then sort by score, a stable sort would leave all faster play times for each score ranked higher, as two duplicate scores wouldn't swap the relative positions they were given from the play time.

* Another property of sorting algorithms is *adaptivity*.
    * An *adaptive* sorting algorithm is one which is able to take advantage of the preexisting order of an array to improve performance.
    * For example, some sorting algorithms are able to quickly detect that an array is already sorted and return much more quickly.

* The final property of a sorting algorithm deals with its memory usage.
    * An *in-place* sorting algorithm uses $O\left(1\right)$ extra (non-recursive) memory. Essentially, besides maybe some swap space or some other fixed number of variables, the only memory being manipulated by the algorithm is the memory that holds the contents of the array being sorted.
    * An *out-of-place* sorting algorithm uses more memory, often times allocating a separate array to place elements as they're being ordered.

* We will see the following sorting algorithms, all of which take $O\left(n^{2}\right)$ comparisons:
    * Bubble Sort
    * Insertion Sort
    * Selection Sort

* In a later note, we will study sorting algorithms which only take $O\left(n \lg n\right)$ comparisons, and we will show that this is actually optimal (meaning that there can be no algorithm which does better).

* Even later, we'll see an algorithm which takes $O\left(n\right)$ time, but we'll have to leave the comparison model behind for that one.

## Bubble Sort

* Bubble sort is the most basic sorting algorithm.
    * The algorithm begins by defining an `endIndex`, initializes it to `array.length - 1`.
    * The algorithm has two layers of loops.
    * The outer loop loops until `endIndex == 0`.
    * The inner loop loops over indices from `0` and stops when it reaches `endIndex` (non-inclusive of `endIndex`).
        * Inside the inner loop, `array[i]` is compared with `array[i+1]`.
        * If the comparison shows that `array[i+1]` should go before `array[i]`, then they are swapped.
    * When the inner loop finishes, `endIndex` is decremented, and it returns to the top of the outer loop.

* The reason the `endIndex` works is because, after one pass through the inner loop, the final element in the array *must* be the final element when the array has been sorted.
    * Each iteration guarantees the element at `endIndex` is at its correct position.
    * Because of this, we only need to check the remaining elements below `endIndex`.
    * This is convenient for making the later iterations of bubble sort much quicker, as we drive the number of comparisons much lower by not comparing with elements guaranteed to be in their correct final positions.

* We can optimize this a bit by adding a `boolean` value `swapOccurred`.
    * We initialize `swapOccurred` to `true` at the beginning of the algorithm.
    * We change the outer loop's condition to check that either `endIndex != 0` or `swapOccurred`.
    * We set `swapOccurred` false at the beginning of the outer loop and run through the inner loop.
    * Whenever a swap occurs in the inner loop, set `swapOccurred` to `true`.
    * This way, even if `endIndex != 0`, if we go through the entire array without swapping anything, then it must be sorted, and we can terminate early.
        * This makes our bubble sort adaptive, because it allows it to terminate pretty much immediately after the first pass through the array when the input was already sorted.

* Another optimization, which is actually more powerful than the previous optimization, is to, instead, use an `int` called `lastSwap` that holds the index of the last index where a swap occurred.
    * Leave the outer loop as it was originally before the previous optimization.
    * Initialize `lastSwap` to the `0` inside the outer loop.
    * Whenever a swap occurs inside the inner loop, set `lastSwap` to the index where the swap occurred.
    * After the inner loop finishes, instead of decrementing `endIndex`, set it to `lastSwap`.
    * We have two cases that show how this is an optimization over the original:
        * If a swap occurs early inside of the array but not later, then `endIndex` may go down much more than a single decrement, as we can guarantee *every* element after `lastSwap` must have been in their correct final position (otherwise, we'd have swapped something that was out of place later in the array).
        * If no swap occurs, then `endIndex` will be `0` after the end of the current outer loop iteration, and so it will terminate just as it would have with the `swapOccurred` optimization.
    * This optimization mimics the previous one when the input array is already sorted, while also reaching an early termination much earlier on mostly, but not entirely, sorted arrays, as we may be able to guarantee a large portion of the array to be sorted already.

* In the following results, we use this optimized form of bubble sort.

* The proofs of these results will come later after all of the iterative sorts have been covered.

**Theorem.** Bubble Sort is correct.

**Theorem.** The best-case number of comparisons for bubble sort is $\Theta\left(n\right)$.

**Theorem.** The worst-case number of comparisons for bubble sort is $O\left(n^{2}\right)$.

**Theorem.** Consider a uniform distribution over all possible arrays of length $n$ from $\left\lbrace 1,\ldots, n \right\rbrace$. Then, the average-case number of comparisons for bubble sort is $\Theta\left(n^{2}\right)$.

The following result is pretty self-evident, and so it will be stated without proof.

**Proposition.** Bubble sort is stable, adaptive, and in-place.

## Insertion Sort

* Insertion sort, like bubble sort, compares adjacent elements.

* Insertion sort maintains two "halves" of the array.
    * The left half is the sorted half.
    * The right half is the yet-to-be sorted half.

* Each step of insertion sort moves an element from the right half to its correct (relative) position among the elements in the left half.

* Insertion sort stops once the left "half" becomes the full array.

* Insertion sort:
    * Loop iterates over `j` from `1` to `array.length - 1`. (Recall: Arrays are 0-indexed).
    * Store `array[j]` inside a variable `nextInsert`.
    * Set `i = j - 1`.
    * While `i >= 0` and `array[i] > nextInsert`:
        * `array[i + 1] = array[i]`
        * Decrement `i`
    * When the `while` loop finishes, set `array[i + 1] = next_insert`.

**Theorem.** Insertion Sort is correct.

**Theorem.** The best-case number of comparisons for insertion sort is $\Theta\left(n\right)$.

**Theorem.** The worst-case number of comparisons for insertion sort is $O\left(n^{2}\right)$.

**Theorem.** Consider a uniform distribution over all possible arrays of length $n$ from $\left\lbrace 1,\ldots, n \right\rbrace$. Then, the average-case number of comparisons for insertion sort is $\Theta\left(n^{2}\right)$.

**Proposition.** Insertion sort is stable, adaptive, and in-place.

## Selection Sort

* Selection sort is a little different than insertion sort and bubble sort.
    * Instead of comparing adjacent items, it compares the (currently known) minimum element among the unsorted elements against the rest of the unsorted elements.
    * If another element is smaller, it saves that index and continues comparing.
    * Once the end of the array is reached, the minimum among the unsorted items is swapped with the left-most element in the unsorted "half" and it starts over.

* Selection sort:
    * `for (int i = 0; i < array.length; i++)`:
        * Set `minIndex = i`.
        * `for (int j = i + 1; j < array.length; j++)`:
            * If the element at `minIndex` is larger than the one at `j`, set `minIndex = j`.
        * When the inner `for` loop finishes, swap the element at index `i` with the one at index `minIndex`.

**Theorem.** Selection Sort is correct.

Interestingly, we have the following theorem which sums up *all* of the performance information about selection sort.

**Theorem.** Selection sort makes exactly $\frac{n\left(n-1\right)}{2}$ comparisons on *any* input of size $n$.

As a corollary, we have the following:

**Corollary.** The best-, worst-, and average-case number of comparisons for selection sort are all $\Theta\left(n^{2}\right)$.

**Proposition.** Selection sort is in-place, but not stable and not adaptive.

**Next: [Analyses of Iterative Sorting Algorithms](./16.InterativeSortAnalyses.md)**