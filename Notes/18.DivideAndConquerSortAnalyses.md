# Analyses of Divide & Conquer Sorting Algorithms

* When doing these analyses, we will use [the master theorem](https://en.wikipedia.org/wiki/Master_theorem_(analysis_of_algorithms)). A proof of the master theorem is beyond the scope of this course.

## Merge Sort

**Theorem.** Merge Sort is correct.

Before proving this, we'll prove a lemma.

**Lemma.** Using `merge` on two sorted subarrays merges them together into a single sorted array.

**Proof.** Suppose we call `merge(A, left, mid, right);` where $A\left[\rm{left}\ldots\rm{mid}\right]$ and $A\left[\rm{mid}+1\ldots\rm{right}\right]$ are sorted. We will maintain the following invariant: All elements copied back into $A$ are in sorted order.

When we do this, we allocate new arrays $L$ and $R$ to store these subarrays. Since the subarrays passed to `merge` were sorted, the arrays $L$ and $R$ are also sorted.

* After storing the subarrays, but before we begin the first iteration of our loop, we set index variables $i = 0$, $j = 0$, and $k = \rm{left}$. At this point, no elements have been copied back, so these 0 elements are vacuously sorted, maintaining the loop invariant.

* Suppose we're on iteration $k$ and assume that we've maintained the loop invariant up to this point. Further assume, without loss of generality, that $L\left[i\right] \leq R\left[j\right]$. Then, we set $A\left[k\right]$ equal to $L\left[i\right]$.

The following three statements must be true:

1. $A\left[\rm{left}\ldots k-1\right]$ is sorted, as we've assumed that we've maintained the loop invariant.
2. $L\left[i\right] \leq R\left[j\right] \leq R\left[j'\right]$ for all $j' > j$, since $R$ is sorted.
3. $L\left[i\right] \leq L\left[i'\right]$ for all $i' > i$, since $L$ is sorted.

These three statements show that $A\left[\rm{left}\ldots k\right]$ is sorted, maintaining the loop invariant.

* When the loop terminates, we'll have started with the loop invariant, maintained the loop invariant at each step, and, therefore, we'll have ended with the loop invariant maintained.

Therefore, when the main loop finishes, we'll have a sorted array. The only steps left come from whatever elements haven't been merged in yet from one of the two subarrays after the other subarray has been fully utilized. Assume, without loss of generality, that it is $L$ that has some elements left.

Since $L$ was sorted, these remaining elements are no smaller than the elements of $L$ that have already been merged in, and since the loop invariant was maintained, all remaining elements of $L$ are no smaller than the elements of $R$ that have been merged in. Therefore, when these remaining elements are added at the end, we will continue to have a sorted array.

**QED**

**Proof of Theorem.**

We proceed by induction on the size of our array, $n$.

Base Case ($n = 1$): In this case, our array is already sorted.

Inductive Case: Suppose merge sort works for all arrays of sizes up to $n\geq 1$.

When merge sort is run on an array of size $n + 1$, it splits the array into two pieces of size $\ell\leq n$ and $r\leq n$ each and recurses. Since merge sort correctly sorts these two subcases, by our inductive hypothesis, and `merge` correctly merges these sorted subarrays into a sorted array, we're done.

Therefore, by induction, merge sort is correct.

**QED**

**Proposition.** Merge Sort is stable, non-adaptive, and out-of-place.

**Proof.**

Merge sort is stable because, when equal elements are encountered, the left-most ones get merged in first.

Merge sort is non-adaptive because it always recurses all the way down and merges back up.

Merge sort is out-of-place because we allocate subarrays in each `merge` call.

**QED**

**Theorem.** The best-, worst-, and average-case number of comparisons for merge sort is $\Theta\left(n \lg n\right)$.

**Proof.**

Since merge sort always recurses and merges regardless of the order of the elements, it must take $\Omega\left(n \lg n\right)$ comparisons in all cases.

Therefore, if we can prove that the worst-case for merge sort is $O\left(n \lg n\right)$, we will have shown that its best- and worst-case number of comparisons is $\Theta\left(n \lg n\right)$, which then shows that its average-case is, as well. Hence, we proceed by proving a worst-case of $O\left(n \lg n\right)$ comparisons.

Note that `merge` makes $O\left(n\right)$ comparisons when given two subarrays whose total size is $n$.

Let $T\left(n\right)$ be the number of comparisons merge sort takes on an array of size $n$ and let $M\left(n\right)$ be the number of comparisons to merge subarrays of total size $n$.

Then,

$$T\left(n\right) = 2 T\left(n/2\right) + M\left(n\right) = 2 T\left(n/2\right) + O\left(n\right).$$

By the master theorem, $T\left(n\right) = \Theta\left(n\lg n\right)$.

**QED**

## Quick Sort

* In the following, if basic or randomized quick sort is not specified, then the theorem is about both.

**Theorem.** Quick sort is correct.

**Proof.**

We proceed by induction on the size of our array, $n$.

Base Case ($n = 1$): In this case, our array is already sorted.

Inductive case: Suppose quick sort works on arrays of any size less than $n$. Then, for an array of size $n$, quick sort starts by splitting it into two subarrays based on the chosen pivot and recurses on each. There are two cases:

1. If our pivot happens to be the largest element, then our first level of recursion simply moves the pivot to the end (if it wasn't already there). Then, we either choose the next pivot at the front (deterministic) randomly select a pivot (randomized), both of which will (at least in expectation, for the randomized case) break our array into two subarrays of size less than $n$.
2. If our pivot wasn't the largest element, then we'll immediately break our array into two subarrays of size less than $n$.

In both cases, we'll (eventually, in the randomized case) be performing quick sort on two arrays of sizes less than $n$, where it works, by our inductive hypothesis. Therefore, quick sort is correct.

**QED**

**Theorem.** The best-case runtime for quick sort is $\Theta\left(n \lg n\right)$.

**Proof.**

At each level of recursion, if we select the median of the subarray as our pivot, we break our subarray in half. Therefore, we get the recurrence

$$T\left(n\right) = 2T\left(\frac{n}{2}\right) + O\left(n\right) = \Theta\left(n \lg n\right).$$

**QED**

**Theorem.** The worst-case number of comparisons for basic quick sort is $\Theta\left(n^{2}\right)$.

**Proof.**

Suppose our input array is in reverse order. Then, each level of recursion moves the front element to the end, and so we recurse $\Theta\left(n\right)$ times. Since each level of recursion takes $n-1$ comparisons, quick sort makes $\Theta\left(n^{2}\right)$ comparisons.

**QED**

**Theorem.** The expected number of comparisons for randomized quick sort is $\leq 2n\ln n$. That is, the worst-case runtime for randomized quick sort is $\Theta\left(n \lg n\right)$ in expectation.

**Proof.**

When a pivot is selected, we split the array up into two subarrays:
1. The left subarray of elements $\leq$ our pivot.
2. The right subarray of elements $>$ our pivot.

Our initial pivot will require $n-1$ comparisons to fill out these subarrays. If our pivot's correct position in the sorted array is $i$, then the left subarray will have $i$ elements and the right $n-i-1$ elements. Since each $i$ has the same $\frac{1}{n}$ probability, we can write a recurrence for the expected number of comparisons made:

$$T\left(n\right) = \left(n-1\right) + \frac{1}{n}\sum_{0\leq i\leq n-1} \left(T\left(i\right) + T\left(n - i - 1\right)\right).$$

Note that $T\left(i\right)$ and $T\left(n - i - 1\right)$ run over the same elements for $i = 0,\ldots, n-1$, so we can group the equal terms together and get rid of the $i=0$ term to get

$$T\left(n\right) = \left(n-1\right) + \frac{2}{n}\sum_{1\leq i\leq n-1}T\left(i\right).$$

Note that $T$ is an increasing function, and so we may bound it above by an integral, $\displaystyle{\sum_{1\leq i\leq n-1}T\left(i\right) \leq \int_{1}^{n} T\left(x\right)\,\rm{d}x}$, which will be helpful in our analysis.

Note that, since our pivot is random, we should "expect" it to be somewhere near the middle. If this happens, we get a recurrence similar to merge sort, so we should expect something like $c\cdot n\ln n$ (we use $\ln$ here for convenience when integrating). We will prove this by induction.

Base case ($n = 1$): In this case, $T\left(1\right) = 0 = c\cdot 1 \ln 1.$

Inductive case: Suppose our guess is correct for each integer below $n$. Then (warning: calculus ahead),

$$T\left(n\right) = \left(n-1\right) + \frac{2}{n}\sum_{1\leq i\leq n-1} c\cdot i \ln i$$

$$\leq \left(n-1\right) + \frac{2}{n}\int_{1}^{n} c\cdot x\ln x\,\rm{d}x =$$

$$\left(n-1\right) + \frac{2}{n}\left(\frac{c}{2}n^{2}\ln n - \frac{cn^{2}}{4} + \frac{c}{4}\right)$$

$$\leq cn \ln n$$

holds for $c = 2$.

Therefore, randomized quick sort makes $O\left(n \lg n\right)$ comparisons in expectation.

**QED**

**Corollary.** The average-case number of comparisons for basic quick sort is $\Theta\left(n \lg n\right)$.

**Proof.**

Since randomized quick sort is like slightly shuffling the input for basic quick sort, the performance of randomized quick sort, in expectation, is the same as considering the average-case performance of basic quick sort.

**QED**

## Comparison Sort Lower Bound

* The following theorem is actually a bit more advanced than what this course goes over, but I feel that it's a very necessary piece to cover before moving to the next sorting algorithm.

**Theorem.** For any deterministic comparison-based sorting algorithm, there exists an input of size $n$ such that $\lg \left(n!\right) = \Omega\left(n \lg n\right)$ comparisons are made.

*Note:* The language used in this proof alludes to some "adversary." This is a way to sort-of anthropomorphize the worst-case input, and is very common in theoretical computer science and game theory. Typically, the adversary has some special powers or knowledge, such as being able to choose the worst option for our algorithm at every step (by way of selecting inputs which conform to those options).

**Proof.**

For an input $\left[a_{1},\ldots, a_{n}\right]$, a deterministic sorting algorithm must find the correct permutation $\sigma\in S_{n}$ such that $\left[a_{\sigma\left(1\right)},\ldots, a_{\sigma\left(n\right)}\right]$ is in sorted order. There are two key notes to make:

1. $\left|S_{n}\right| = n!$
2. For each $\sigma \in S_{n}$, there exists an input $\left[a_{1},\ldots, a_{n}\right]$ such that $\left[a_{\sigma\left(1\right)},\ldots, a_{\sigma\left(n\right)}\right]$ is in sorted order (just take the sorted order and reverse that permutation to get the correct input), and can be the *only* correct permutation, if we use distinct elements (that is, $a_{i} \neq a_{j}$ for $i\neq j$). This means, assuming our input contains distinct elements, we have a bijection between all orderings of our input and the permutations that sort them.

Therefore, we need only consider all permutations of a set of distinct elements, say $\left\lbrace 1,\ldots, n\right\rbrace$.

For any given input, let $T_{k}$ be the set of inputs that are consistent how our algorithm has responded on comparison $k$. That is, $T_{0}$ is the set of all possible inputs ($\left|T_{0}\right| = n!$), then $T_{1}$ is the set of inputs whose answer to the first comparison question match the answer of the first comparison on our input. Then, each comparison splits $T_{k}$ into two groups based on the answer to comparison number $k+1$.

Now, suppose there is an adversary who wants to make our algorithm take as long as possible. Because this adversary knows exactly how to answer each comparison so that $T_{k}$ is as large as possible, they pick an input that matches this. Therefore, the most optimal deterministic algorithm to combat this would be one which splits the set of possible inputs in half at each step, as this minimizes the size of the larger of the two groups.

Since we're halving the number of possible permutations at each step, and we started with $n!$ possible permutations when we started, this means we make at least $\lg \left(n!\right) = \Omega\left(n \lg n\right)$ comparisons.

**QED**

**Corollary. [Comparison Sort Lower Bound]** Every comparison-based sorting algorithm makes $\Omega\left(n \lg n\right)$ comparisons in the worst case.

* We can actually do better than this! We can get an *average-case* lower bound, and then turn that into a lower bound including *randomized* (comparison-based) sorting algorithms!

**Theorem.** For any deterministic comparison-based sorting algorithm, the average-case number of comparisons is $\Omega\left(n \lg n\right)$.

**Proof.**

Consider the set of all $n!$ orderings of an input array of $n$ distinct elements. Instead of considering an adversarial approach, we will build what's known as a *decision tree* for a deterministic comparison-based sorting algorithm.

We build the decision tree by considering all yes/no answers to our comparisons when running our algorithm on every input. This gives a tree with $n!$ leaves, where leaves correspond to the final state of our array after sorting. Recall that a balanced BST with $k$ leaves has $\Theta\left(\lg k\right)$ depth at all leaves, so all we need to show is that the minimum average depth is achieved by an algorithm which has a completely balanced decision tree.

Given an algorithm with an unbalanced decision tree, take two leaves at the largest depth who share a parent and move them to be leaves of a node at the shortest depth (this, essentially, changes which algorithm we're using). Since the difference in depths between the largest and smallest was at least 2 (otherwise, the tree was balanced already), this reduces the average depth of our leaves.

Therefore, the average depth of a decision tree for a deterministic comparison-based sorting algorithm is minimized when it's balanced, and so the the average-case number of comparisons is $\Omega\left(n \lg n\right)$.

**QED**

* Now, we can modify this proof just a bit more to get the same result for randomized comparison-based sorting algorithms.

**Theorem.** For any randomized comparison-based sorting algorithm, the average-case number of comparisons is $\Omega\left(n \lg n\right)$.

* Before proving this, we'll talk about how we can deal with randomized algorithms, in general.

* We may consider a particular instance of a randomized algorithm to be a deterministic algorithm where the randomized bits used by that instance were are hard-coded.
    * Therefore, a randomized algorithm is just a probability distribution over all such deterministic algorithms.

* If we call our randomized algorithm $A$ and, given a string $r$ of random bits, we call the corresponding deterministic algorithm with those bits hard-coded in $A_{r}$, we get the expected runtime of $A$ on an input $x$ by

$$\sum_{r} \mathbb{P}\left[r\right] \mathrm{cost}\left(A_{r}, x\right).$$

* Hence, we get our result by analyzing

$$\underset{x}{\mathbb{E}}\left[\sum_{r} \mathbb{P}\left[r\right] \mathrm{cost}\left(A_{r}, x\right)\right],$$

where $x$ is uniformly distributed.

* Now, we can prove out theorem.

**Proof.**

For a randomized comparison-based sorting algorithm, the expected number of comparisons is

$$\underset{x}{\mathbb{E}}\left[\sum_{r} \mathbb{P}\left[r\right] \mathrm{cost}\left(A_{r}, x\right)\right]$$

$$= \sum_{r} \mathbb{P}\left[r\right] \underset{x}{\mathbb{E}}\left[\mathrm{cost}\left(A_{r}, x\right)\right]$$

$$\geq \sum_{r} \mathbb{P}\left[r\right] \Omega\left(n \lg n\right) = \Omega\left(n \lg n\right),$$

as required.

**QED**

* Recall that big-$\Omega$ is a *lower* bound; we can do worse, such as the worst-case for bubble sort.
    * What this tells us is that, asymptotically, we can't do better than merge sort or quick sort, at least when we're working with comparison-based sorting.

* There are non-comparison-based sorting algorithms that can do better in specific circumstances.
    * In fact, one of them (LSD Radix Sort) is covered in the course that these notes are based on.
    * However, for the first time in these notes, I will be skipping some content due to time constraints.

**Next: [Pattern Matching Algorithms I: Boyer-Moore](./19.BoyerMoore.md)**