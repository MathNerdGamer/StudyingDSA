### Stacks & Queues

#### Stacks
* A *stack* is a linear ADT.

* Linear ADTs are finite collections where each object has one immediate predecessor and one immediate successor.

* Such ADTs may be implemented with arrays or linked lists.

* A *stack* has a *push* operation, where elements are placed on to the top, and *pop*, where elements are taken off the top. Since we only deal with the top at any given time, we do *not* support search or arbitrary index access, insertion, or removal.
    * Because of the way stacks work, they are known as a **L**ast **I**n, **F**irst **O**ut (**LIFO**) model.

* Recall the notion of a *call stack*.
    * When a method is called, the current method that is active is put on pause, and the new method that was just called begins execution.
    * When a method call terminates, the method call that was previously active unpauses and continues execution from where the method call was made.

* For the method call stack, making a method call translates to pushing a method call onto the stack, along with any parameters and information needed to pause and begin execution. Terminating a method call corresponds to popping a method call from the stack. The method that is currently executing is whatever method call is currently at the top of the stack.

##### Stack ADT
The following is the specification of the **Stack ADT**:

* `void push(T)`: Element of type `T` is pushed to the top of a the stack.
* `T pop()`: Element of type `T` on top of the stack is popped off the stack and returned.
* `T top()`: Element of type `T` on top of the stack is returned without being popped.
* `boolean isEmpty()`: Returns `true` if the stack is empty, `false` if not.
* `void clear()`: Removes everything from the stack.
* `int size()`: Returns the number of elements currently in the stack.

Suppose we have a `SinglyLinkedList` data structure with the usual operations. Then, we have the following implementation of a stack, along with the complexity from the table from [Linked Lists](./5.LinkedLists.md).

* A `SinglyLinkedList` data member called `list`.
* `void push(T x)`: Implemented as `list.addToHead(x)`. $O\left(1\right)$
* `T pop()`: Implemented as `return list.removeFromHead()`. $O\left(1\right)$
* `T top()`: Implemented as `return list.accessHead()`. $O\left(1\right)$
* `boolean isEmpty()`: Implemented as `return list.isEmpty()`. $O\left(1\right)$
* `void clear()`: Implemented as `list.clear()`. $O\left(1\right)$
* `int size()`: Implemented as `return list.size()`. $O\left(1\right)$

Suppose, instead, that we use a dynamic array. Then, we have the following implementation of a stack, along with the complexity from the table from [Linked Lists](./5.LinkedLists.md).

* An `ArrayList` data member called `list`.
* `void push(T x)`: Implemented as `list.add(x)`. $O\left(1\right)$ *amortized*
* `T pop()`: Implemented as `return list.remove(list.size() - 1)`. $O\left(1\right)$
* `T top()`: Implemented as `return list.get(list.size() - 1)`. $O\left(1\right)$
* `boolean isEmpty()`: Implemented as `return list.isEmpty()`. $O\left(1\right)$
* `void clear()`: Implemented as `list.clear()`. $O\left(1\right)$
* `int size()`: Implemented as `return list.size()`. $O\left(1\right)$

Note that the `SinglyLinkedList` version actually *wins* in time complexity, because having a per-operation $O\left(1\right)$ complexity is a better guarantee than just an amortized $O\left(1\right)$ complexity that might be hiding one really bad case through averaging.

<table border="1">
    <thead>
        <tr>
            <td><strong>Data Structure</strong></td>
            <td><strong>Top of the Stack</strong></td>
            <td><strong>push()</strong></td>
            <td><strong>pop()</strong></td>
            <td><strong>isEmpty()</strong></td>
            <td><strong>size()</strong></td>
            <td><strong>clear()</strong></td>
            <td><strong>Changing Size</strong></td>
        </tr>
    </thead>
    <tbody>
        <tr>
            <td>Singly-Linked List</td>
            <td>head</td>
            <td>$O\left(1\right)$</td>
            <td>$O\left(1\right)$</td>
            <td>$O\left(1\right)$</td>
            <td>$O\left(1\right)$</td>
            <td>$O\left(1\right)$</td>
            <td>$O\left(1\right)$</td>
        </tr>
        <tr>
            <td>Dynamic Array</td>
            <td>Index size-1</td>
            <td>$O\left(1\right)$ *am.* / $O\left(n\right)$ worst-case</td>
            <td>$O\left(1\right)$</td>
            <td>$O\left(1\right)$</td>
            <td>$O\left(1\right)$</td>
            <td>$O\left(1\right)$</td>
            <td>$O\left(n\right)$</td>
        </tr>
    </tbody>
</table>


#### Queues (Queue ADT)

A *queue* is similar to a stack, except that elements are removed in the order that they're inserted. In other words, they're a **F**irst **I**n, **F**irst **O**ut (**FIFO**) model. A good way to imagine it is like a queue in real life.

Because they are so similar to stacks, the following specification of the **Queue ADT** shouldn't be surprising.

* `void enqueue(T)`: Element of type `T` is added to the back of a the queue.
* `T dequeue()`: Element of type `T` at the front of the queue is removed and the value is returned.
* `T top()`: Element of type `T` at the front of the queue is returned without being dequeued.
* `boolean isEmpty()`: Returns `true` if the queue is empty, `false` if not.
* `void clear()`: Removes everything from the queue.
* `int size()`: Returns the number of elements currently in the queue.

As you make imagine, this can be easily modeled with a `SinglyLinkedList`, with the following complexities:

* A `SinglyLinkedList` data member called `list`.
* `void enqueue(T)`: Implemented as `list.addToTail(x)`. $O\left(1\right)$
* `T dequeue()`: Implemented as `return list.removeFromHead()`. $O\left(1\right)$
* `T top()`: Implemented as `return list.accessHead()`. $O\left(1\right)$
* `boolean isEmpty()`: Implemented as `return list.isEmpty()`. $O\left(1\right)$
* `void clear()`: Implemented as `list.clear()`. $O\left(1\right)$
* `int size()`: Implemented as `return list.size()`. $O\left(1\right)$

The only difference between a queue and a stack, in implementation, is that we use the `addToTail()` method of `SinglyLinkedList` instead of `addToHead()`. In fact, the rest of the implementation is *identical* in content.

On the other hand, implementing with an array can cause more complications. Instead of using an `ArrayList` directly, we'll need to use something a tad bit more complex. Let's go over a high-level overview of how the implementation works.

* An array data member called `list`.
* Four `int` data members called `size`, `capacity`, `front`, and `back`.
* The `size` member holds the number of elements in the array, `capacity` holds the total number of elements that can be stored without resizing, `front` holds the index to the first element, and `back` holds the index to the last element. All values initially equal `0`.
* `void enqueue(T)`: Implemented by storing the element at the `back` index and incrementing `size`.
    * When the element is added, increment the `back` index by `1` and modulo by `capacity`.
    * An element is being added beyond the `capacity` can be detected by checking if `size == capacity` before insertion.
        * If not equal, then the element can be added as usual.
        * If they are equal, resize the array, re-`enqueue` everything in order, and then insert as usual.
    * In total, this operation is still $O\left(1\right)$ *amortized* complexity.
* `T dequeue(T)`: Implemented by removing the element stored at `front` index, incrementing `front` modulo `capacity`, and returning the element that was removed. $O\left(1\right)$
* `T top()`: Implemented by returning the element stored at `front`. $O\left(1\right)$
* `boolean isEmpty()`: Implemented as `return size == 0`. $O\left(1\right)$
* `int size()`: Implemented as `return size`. $O\left(1\right)$

We get the same table of complexities as before, but now we should ask ourselves: Why did we bother with the array implementations if they're techniquely worse in complexity?

#### Locality of Reference
* **This is my favorite concept in Computer Science!**

* Memory is actually a hierarchy of larger/slower and smaller/faster memory, where the closer you get to the CPU, the smaller (but faster) the memory gets. One of the great bottlenecks in computing is memory latency and throughput, with a lot (actually, a shocking amount) of time being spent just waiting for data to arrive before being processed by the CPU.

* Because of this, we need to be able to pull in more data than actually required so as to (hopefully) spend more time actually processing rather than waiting. However, we don't know the future, so we have to employ heuristics when collecting data from memory beyond the data immediately requested.

* One of the greatest heuristics is that of **Locality of Reference**. The idea actually comes in two (highly) related flavors:
    * *Spatial locality*: If you need some data, then it's likely that you'll also need some of the nearby data. Example: Looping over an array.
    * *Temporal locality*: If you need some data now, then it's likely that you'll also need it again soon. Example: The index used when looping.
    * These two flavors of locality do typically work very well in practice, and, hence, many CPUs employ them when making decisions about how to fill their cache.

* It would take way too long to go over this idea in more detail (such as cache eviction algorithms like **LRU**, the various analyses of these algorithms, such as competitive analysis [including resource augmentation] or parameterized analysis, or why it's better to request blocks instead of individual memory cells in the first place), so let's just say that we're working on a system that uses these heuristics.

* An array is *great* for locality, because everything is stored in a contiguous block. That means, when one element of the array is requested, it's very likely that other elements of the array are stored in the same block. That means there's less time spent by a CPU waiting for data to process, because they will have already been requested and stored in cache when the CPU needs them (because the CPU requests the entire block).

* A linked list, on the other hand, consists of some loosely connected nodes that need not be anywhere near each other in memory. This means that a bunch of time is spent waiting for the data to reach the CPU to process, and a lot of *irrelevant* data may be brought into cache just because that data happens to reside in the same block of memory that the node was stored in.

* In fact, the *utter lack* of locality afforded by linked lists often *more* than offsets the asymptotically better guarantees they make in theory. So much so that, in languages like C++ where performance is much more emphasized, it is often said to *always* use `std::vector`, unless you know what you're doing (and, even if you do, *use `std::vector`*).

There is so much more than could be said about locality of reference, but I'll leave that to a high-performance computing course (or an algorithms course like [Beyond Worst-Case Analysis](https://timroughgarden.org/f14/f14.html), which I highly recommend -- lectues 3 & 4 hold the relevant material, but it's far beyond the scope of the present course).

**Moral of the story:** Remember that analysis tools are just that, tools. One should equip their self with many tools and understand the pros and cons of each one before making a final decision.

#### Variants of Stacks & Queues

##### Priority Queues

* Recall that the Stack ADT removes the most recently added element when the `pop()` method is called, and the Queue ADT removes the least recently added element with `dequeue()`. The *Priority Queue ADT*, however, removes elements in order of their *priority*.

* Defining what the priority of elements are based on gives many different forms to a priority queue for any given set of data. For example, we may define priority in terms of the size of integer values, where either larger values or smaller values get higher priority. This changes the order that elements are removed when `dequeue()` is called, but the underlying idea is the same.

* Our current data structure knowledge is not enough to get an efficient priority queue implementation, because we require some way of sorting efficiently. With our current knowledge, we'd inevitably run into $O\left(n\right)$ operations for `enqueue()`.
    * For an array-based Priority Queue, when `enqueue()` is called, we'd have to shift elements around to maintain the sorted order based on the priorities of the elements, which means we'd have to move up to $O\left(n\right)$ elements in the worst case.
    * For a linked list-based Priority Queue, when `enqueue()` is called, we would have to search for the correct position, which requires traversing the `next` / `prev` pointers up to $O\left(n\right)$ times in the worst case.

* An efficient implementation will be seen in subsequent material.

#### Deques (Deque ADT)

The *double-ended queue* (or *deque*) interface specifies operations for adding/removing from both ends, and can be thought of as being a stack and a queue at the same time.

The **Deque ADT** is given by the following:

* `void addFirst(T)`: Element of type `T` is added to the front of a the deque.
* `void addLast(T)`: Element of type `T` is added to the back of a the deque.
* `T removeFirst()`: Element of type `T` at the front of the deque is removed and the value is returned.
* `T removeLast()`: Element of type `T` at the end of the deque is removed and the value is returned.
* `T top()`: Element of type `T` at the front of the queue is returned without being removed.
* `T bottom()`: Element of type `T` at the end of the queue is returned without being removed.
* `boolean isEmpty()`: Returns `true` if the queue is empty, `false` if not.
* `void clear()`: Removes everything from the queue.
* `int size()`: Returns the number of elements currently in the queue.

The implementation using an array is similar to the array implementation of a queue. In fact, the only really new parts are the `addFirst()` and `removeLast()` operations (`bottom()` is simple, so we don't need to think about it). However, it turns out that they mirror the `addLast()` and `removeFirst()` operations, but with the `front` and `back` indices swapping places.

Implementing with a linked-list is also simple, as long we we now include `prev` pointers (turning it into a doubly linked-list).

**Next: [Trees](./7.Trees.md)**